{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be5e7a1a7278c138e277e90b019db4961154c731"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "from operator import itemgetter    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import funcsigs\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV,KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, auc\n",
    "from funcsigs import signature\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "from statsmodels.formula import api\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d624986c6d662b3dec980c593981fb913cb00c01"
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "\n",
    "df = pd.read_excel(r\"dataset.xlsx\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set target and features\n",
    "\n",
    "target = 'COPD_Diagnose_ja_nein'\n",
    "features = [i for i in df.columns if i not in [target]]\n",
    "\n",
    "# make a copy of the original data\n",
    "\n",
    "original_df = df.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show what are in the work space.\n",
    "\n",
    "dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fce3fd763d30cf516d00815bb386cd95cea14ec",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display some data information \n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fce3fd763d30cf516d00815bb386cd95cea14ec",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display some data information \n",
    "\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fce3fd763d30cf516d00815bb386cd95cea14ec",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display some data information \n",
    "\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the number of values for all columns \n",
    "\n",
    "df.nunique().sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figure out numerical and categorical features\n",
    "\n",
    "nu = df[features].nunique().sort_values()\n",
    "nf = []; \n",
    "cf = []; \n",
    "nnf = 0; \n",
    "ncf = 0; \n",
    "\n",
    "for i in range(df[features].shape[1]):\n",
    "    if nu.values[i]<=16: \n",
    "        cf.append(nu.index[i])\n",
    "    else: nf.append(nu.index[i])\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Datset has {} numerical and {} categorical features.'.format(len(nf),len(cf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set how many numerical features and categorical features.\n",
    "\n",
    "Nnf = 13\n",
    "Ncf = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count plot for categorical features\n",
    "\n",
    "print('\\033[1mVisualising Categorical Features:'.center(100))\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 2, ncols = 3)\n",
    "ax = axs.flatten()\n",
    "for i in range(len(cf)):\n",
    "    if df[cf[i]].nunique()<=16:\n",
    "        sns.countplot(x = cf[i], data = df, ax = ax[i])\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot for numerical features\n",
    "\n",
    "print('\\033[1mNumeric Features Distribution'.center(130))\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 5, ncols = 3)\n",
    "ax = axs.flatten()\n",
    "for i in range(len(nf)):\n",
    "    sns.distplot(df[nf[i]], ax = ax[i], hist_kws=dict(edgecolor=\"black\", linewidth=2), bins=10)\n",
    "plt.tight_layout()\n",
    "ax[13].set_visible(False)\n",
    "ax[14].set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot for numerical features\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 5, ncols = 3)\n",
    "ax = axs.flatten()\n",
    "for i in range(len(nf)):\n",
    "    df.boxplot(nf[i], ax = ax[i])\n",
    "plt.tight_layout()\n",
    "ax[13].set_visible(False)\n",
    "ax[14].set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplots for all features \n",
    "\n",
    "# g = sns.pairplot(df)\n",
    "# plt.title('Pairplots for all the Feature')\n",
    "# g.map_upper(sns.kdeplot, levels=4, color=\".2\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of Null values in each feature\n",
    "\n",
    "nvc = pd.DataFrame(df.isnull().sum().sort_values(), columns=['Total Null Values'])\n",
    "nvc['Percentage'] = round(nvc['Total Null Values']/df.shape[0],3)*100\n",
    "print(nvc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review a few rows of the data\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the null values in a few features by the most frequent value in that column. \n",
    "\n",
    "df['Raceethnicity'].fillna(df['Raceethnicity'].mode()[0],inplace=True)\n",
    "df['Sex'].fillna(df['Sex'].mode()[0],inplace=True)\n",
    "df['Dentatestatus'].fillna(df['Dentatestatus'].mode()[0],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the null values in a some features by mean value.  \n",
    "\n",
    "col_to_be_imputed = ['AgeatinterviewScreener', 'PovertyIncomeRatiounimputedincome', 'Pulseratebeatsminage5years','OverallaverageK1systolicBPage5', 'OverallaverageK5diastolicBPage5', 'Bodymassindex','Upperquadrantperiodontalassessments','Lowerquadrantperiodontalassessments','SumofpermanentDMFSduetodisease','SumofpermanentDFMSduetoanycause','SumofpermanentDFMSduetoanycause','SumofpermanentDFS','SumofpermanentDMFTduetodisease','SumofpermanentDMFTduetoanycause','Bleeding_Percentage','Furcation_SUM', 'MAL']\n",
    "for item in col_to_be_imputed:\n",
    "    df[item].fillna(df[item].mean(),inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder categrical features by LabelEncoder. \n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Raceethnicity'] = le.fit_transform(df['Raceethnicity'])\n",
    "df['Sex'] = le.fit_transform(df['Sex'])\n",
    "df['Dentatestatus'] = le.fit_transform(df['Dentatestatus'])\n",
    "\n",
    "# convert other features values from float to int\n",
    "\n",
    "df['AgeatinterviewScreener'] = df['AgeatinterviewScreener'].astype(int)\n",
    "df['PovertyIncomeRatiounimputedincome'] = df['PovertyIncomeRatiounimputedincome'].astype(int)\n",
    "df['Pulseratebeatsminage5years'] = df['Pulseratebeatsminage5years'].astype(int)\n",
    "df['OverallaverageK1systolicBPage5'] = df['OverallaverageK1systolicBPage5'].astype(int)\n",
    "df['OverallaverageK5diastolicBPage5'] = df['OverallaverageK5diastolicBPage5'].astype(int)\n",
    "df['Bodymassindex'] = df['Bodymassindex'].astype(int)\n",
    "df['Upperquadrantperiodontalassessments'] = df['Upperquadrantperiodontalassessments'].astype(int)\n",
    "df['Lowerquadrantperiodontalassessments'] = df['Lowerquadrantperiodontalassessments'].astype(int)\n",
    "df['SumofpermanentDMFSduetodisease'] = df['SumofpermanentDMFSduetodisease'].astype(int)\n",
    "df['SumofpermanentDFMSduetoanycause'] = df['SumofpermanentDFMSduetoanycause'].astype(int)\n",
    "df['SumofpermanentDFMSduetoanycause'] = df['SumofpermanentDFMSduetoanycause'].astype(int)\n",
    "df['SumofpermanentDFS'] = df['SumofpermanentDFS'].astype(int)\n",
    "df['SumofpermanentDMFTduetodisease'] = df['SumofpermanentDMFTduetodisease'].astype(int)\n",
    "df['SumofpermanentDMFTduetoanycause'] = df['SumofpermanentDMFTduetoanycause'].astype(int)\n",
    "df['Bleeding_Percentage'] = df['Bleeding_Percentage'].astype(int)\n",
    "df['Furcation_SUM'] = df['Furcation_SUM'].astype(int)\n",
    "df['MAL'] = df['MAL'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information of all features. \n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some useless features. \n",
    "\n",
    "df_feat = df.drop(['Raceethnicity','Sex','Dentatestatus','COPD_Diagnose_ja_nein'],axis=1)\n",
    "\n",
    "# get labels for each sample.\n",
    "\n",
    "target =  pd.get_dummies(df[['COPD_Diagnose_ja_nein']],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels for each categrical feature. \n",
    "\n",
    "dummies = pd.get_dummies(df[['Raceethnicity','Sex','Dentatestatus', ]],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a few rows of the useful features. \n",
    "\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize all features data by sklearn StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_feat)\n",
    "df_scaled = scaler.transform(df_feat)\n",
    "df_scaled = pd.DataFrame(df_scaled,columns=df_feat.columns[:19])\n",
    "df_preprocessed = pd.concat([df_scaled,dummies,target], axis=1)\n",
    "df_preprocessed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of features and number of samples\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df_preprocessed.shape[1], df_preprocessed.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summarize of feature values\n",
    "\n",
    "df_preprocessed.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "798b9834d32ba534e5aca36d5763b9624fe00565"
   },
   "outputs": [],
   "source": [
    "# set y (values to be predicted), and X (feature values)\n",
    "\n",
    "y = df_preprocessed['COPD_Diagnose_ja_nein']\n",
    "X = df_preprocessed.drop('COPD_Diagnose_ja_nein', axis = 1)\n",
    "\n",
    "print(\"y - Labels\", y.shape)\n",
    "print(\"X - N Label N id \", X.shape)\n",
    "print(X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60be44d230f432f3f087f15a238feb21deb68bbf"
   },
   "outputs": [],
   "source": [
    "# review the shapes of data frames\n",
    "\n",
    "print(df_preprocessed.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aefe400392c73d77e55bf9f2410e7b725df2e77c"
   },
   "outputs": [],
   "source": [
    "# separate data to Tain or Test.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "print ('X_train: ', X_train.shape)\n",
    "print ('X_test: ', X_test.shape)\n",
    "print ('y_train: ', y_train.shape)\n",
    "print ('y_test: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f03ccf0923716b07035356abcf3acb4a51d23025"
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "\n",
    "X_T = X.transpose()\n",
    "corr_A = X_T.corr()\n",
    "\n",
    "def co(x):\n",
    "    if x < 0: return 0 \n",
    "    else: return x\n",
    "    \n",
    "corr_W = corr_A.applymap(co)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set train and test\n",
    "\n",
    "Y = y\n",
    "\n",
    "perm = torch.randperm(len(Y))\n",
    "train_mask = perm[:round(len(Y)*0.8)]\n",
    "val_mask = perm[round(len(Y)*0.8):round(len(Y)*0.9)]\n",
    "test_mask = perm[round(len(Y)*0.9):]\n",
    "train_mask = train_mask.new_full((len(Y),), False, dtype=torch.bool)\n",
    "train_mask[perm[:round(len(Y)*0.8)]] = True\n",
    "val_mask = val_mask.new_full((len(Y),), False, dtype=torch.bool)\n",
    "val_mask[perm[round(len(Y)*0.8):round(len(Y)*0.9)]] = True\n",
    "test_mask = test_mask.new_full((len(Y),), False, dtype=torch.bool)\n",
    "test_mask[perm[round(len(Y)*0.9):]] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data and import packages\n",
    "\n",
    "adj = corr_W.to_numpy()\n",
    "sparse = scipy.sparse.coo_matrix(adj)\n",
    "row = sparse.row\n",
    "col = sparse.col\n",
    "ww = sparse.data\n",
    "edge_index = torch.stack([torch.LongTensor(row), torch.LongTensor(col)])\n",
    "edge_weight = torch.FloatTensor(ww)\n",
    "\n",
    "\n",
    "x = X.to_numpy()\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "y = Y.to_numpy()\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight, y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "data.x = data.x.float()\n",
    "data.y = data.y.long()\n",
    "data.edge_weight = data.edge_weight.float()\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a GNN class\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, shared_weights=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(19, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device and optimizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(hidden_channels=19, shared_weights=True, dropout=0.3).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a train function\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "#--    out = model(data.x, data.adj_t)\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a test function\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "#--    pred, accs = model(data.x, data.adj_t).argmax(dim=-1), []\n",
    "    pred, accs = model(data).argmax(dim=-1), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run GNN and print results\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "times = []\n",
    "for epoch in range(1, 200):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
    "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
    "          f'Final Test: {test_acc:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "30231eece09b4e58137f7de92a1edff1ce67d1eb6dc3e77c83046d0c2c0004f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
